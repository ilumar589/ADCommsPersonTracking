# YOLO11 Integration Tests

This document describes the YOLO11 ONNX model integration tests using Testcontainers.

## Overview

The integration tests in `ADCommsPersonTracking.Tests/Integration/YoloIntegrationTests.cs` validate the YOLO11 person detection functionality by:

1. Spinning up a Docker container with the actual YOLO11 ONNX model
2. Running real inference on test images
3. Validating detection results and bounding box outputs

## Prerequisites

### Required Software
- Docker Desktop or Docker Engine (must be running)
- .NET 10 SDK
- YOLO11 ONNX model (yolo11n.onnx)

### Download the YOLO11 Model

Before running the integration tests, you need to download the YOLO11 ONNX model:

#### Option 1: Using Python Script (Recommended)
```bash
# Install ultralytics
pip install ultralytics

# Run the download script
python download-model.py
```

#### Option 2: Manual Download with Ultralytics
```bash
pip install ultralytics
python -c "from ultralytics import YOLO; model = YOLO('yolo11n.pt'); model.export(format='onnx', simplify=True, dynamic=False, imgsz=640)"
mv yolo11n.onnx models/
```

The model will be saved to `models/yolo11n.onnx` (~11MB).

## Running the Integration Tests

**Important**: Ensure Docker is running before executing integration tests.

### All Integration Tests
```bash
# Run only integration tests by category
dotnet test --filter "Category=Integration"

# Run with detailed output
dotnet test --filter "Category=Integration" --logger "console;verbosity=detailed"

# Alternative: Run by namespace
dotnet test --filter "FullyQualifiedName~Integration"
```

### Specific Test
```bash
dotnet test --filter "FullyQualifiedName~YoloIntegrationTests.Container_HealthCheck_ReturnsHealthy"
```

### Unit Tests Only (Excluding Integration)
```bash
# Skip integration tests (useful for CI where Docker might not be available)
dotnet test --filter "Category!=Integration"

# Or exclude by namespace
dotnet test --filter "FullyQualifiedName!~Integration"
```

### All Tests (Unit + Integration)
```bash
dotnet test
```

## Test Scenarios

The integration test suite covers the following scenarios:

### 1. Container Health Check
- **Test**: `Container_HealthCheck_ReturnsHealthy`
- **Purpose**: Verifies the Docker container starts correctly and the model loads
- **Expected**: Health endpoint returns "healthy" status with model_loaded=true

### 2. Person Detection
- **Test**: `Detect_WithImageContainingPerson_DetectsPerson`
- **Purpose**: Tests detection on an image with a person-like shape
- **Expected**: Valid response with detection structure (may or may not detect the simple shape)

### 3. No Person Detection
- **Test**: `Detect_WithImageWithoutPerson_ReturnsNoDetections`
- **Purpose**: Validates that solid color images don't trigger false person detections
- **Expected**: Zero detections

### 4. Empty Scene Detection
- **Test**: `Detect_WithEmptyScene_ReturnsNoDetections`
- **Purpose**: Validates that gradient/empty scenes don't trigger false detections
- **Expected**: Zero detections

### 5. Confidence Threshold Filtering
- **Test**: `Detect_WithCustomConfidenceThreshold_FiltersDetections`
- **Purpose**: Tests that confidence threshold parameter works correctly
- **Expected**: Only detections above threshold are returned

### 6. Bounding Box Format
- **Test**: `Detect_BoundingBoxFormat_IsCorrect`
- **Purpose**: Validates the structure and format of detection results
- **Expected**: All required fields present with valid values (x, y, width, height, confidence, label, class_id)

### 7. Model Information
- **Test**: `ModelInfo_ReturnsCorrectInformation`
- **Purpose**: Verifies model metadata is accessible
- **Expected**: Correct input/output shape information (YOLO11: [1, 84, 8400])

## Test Images

Test images are located in `ADCommsPersonTracking.Tests/TestData/Images/`:

- `person.jpg` - Simple person-like shape (blue shirt, skin-tone head, black legs)
- `no_person.jpg` - Solid blue background
- `empty_scene.jpg` - Gradient background

These images are generated by the `create-test-images.py` script.

## Docker Container Architecture

The integration tests use a Python Flask server running in a Docker container:

- **Base Image**: `python:3.12-slim`
- **Model**: YOLO11n ONNX (mounted from host)
- **Runtime**: ONNX Runtime 1.20.1
- **Endpoints**:
  - `GET /health` - Health check
  - `POST /detect` - Perform detection (accepts image bytes)
  - `GET /info` - Model information

### Container Files
- `Dockerfile.yolo` - Docker image definition
- `yolo_inference_server.py` - Flask server for inference
- `models/yolo11n.onnx` - YOLO11 model (not committed, generated)

## CI/CD Considerations

### GitHub Actions

The integration tests can run in GitHub Actions with Docker support:

```yaml
jobs:
  integration-tests:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      
      - name: Setup .NET
        uses: actions/setup-dotnet@v4
        with:
          dotnet-version: '10.0.x'
      
      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
      
      - name: Download YOLO11 model
        run: |
          pip install ultralytics
          python download-model.py
      
      - name: Run integration tests
        run: dotnet test --filter "FullyQualifiedName~Integration"
```

**Note**: Docker is pre-installed on GitHub Actions Ubuntu runners, so no additional setup is needed for Testcontainers.

## Troubleshooting

### Docker Not Running
```
Error: Cannot connect to the Docker daemon
```
**Solution**: Ensure Docker Desktop or Docker Engine is running.

### Model Not Found
```
Warning: Model not found at /app/models/yolo11n.onnx
```
**Solution**: Run `python download-model.py` to download the model before running tests.

### Port Already in Use
```
Error: Port 5000 is already allocated
```
**Solution**: Testcontainers uses dynamic port mapping, but if you see this error, stop any services using port 5000.

### Container Build Fails
```
Error: Docker build failed
```
**Solution**: Ensure `Dockerfile.yolo` and `yolo_inference_server.py` are present in the repository root.

### Tests Timeout
```
TimeoutException: Container did not start within the specified timeout
```
**Solution**: 
- Check Docker has enough resources (CPU, memory)
- Increase timeout in test configuration
- Verify network connectivity for image pulls

## Performance Notes

- **First Run**: Slower due to Docker image building (~2-3 minutes)
- **Subsequent Runs**: Faster as Docker caches the image (~30-60 seconds per test)
- **Model Loading**: Takes ~5-10 seconds in container startup
- **Inference**: ~100-500ms per image on CPU

## Local Development

For faster iteration during development, you can run the inference server locally:

```bash
# Install dependencies
pip install flask onnxruntime numpy pillow

# Run server
python yolo_inference_server.py

# Test manually
curl -X POST -H "Content-Type: image/jpeg" --data-binary @test.jpg http://localhost:5000/detect
```

## Additional Resources

- [Testcontainers for .NET Documentation](https://dotnet.testcontainers.org/)
- [YOLO11 Documentation](https://docs.ultralytics.com/)
- [ONNX Runtime Documentation](https://onnxruntime.ai/)
- [Docker Documentation](https://docs.docker.com/)
